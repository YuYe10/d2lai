{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. 生成数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.411269Z",
     "start_time": "2025-02-19T01:32:59.934113Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. 读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.417878Z",
     "start_time": "2025-02-19T01:33:01.415322Z"
    }
   },
   "source": [
    "def load_array(data_arrays, batch_size, is_train=True):  #@save\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.471701Z",
     "start_time": "2025-02-19T01:33:01.468507Z"
    }
   },
   "source": [
    "# nn是神经网络的缩写\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential(nn.Linear(2, 1))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4. 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.517733Z",
     "start_time": "2025-02-19T01:33:01.512586Z"
    }
   },
   "source": [
    "net[0].weight.data.normal_(0, 0.01)\n",
    "net[0].bias.data.fill_(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.5. 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.562943Z",
     "start_time": "2025-02-19T01:33:01.560119Z"
    }
   },
   "source": [
    "\"\"\" loss = nn.MSELoss() \"\"\"\n",
    "\"\"\" loss = nn.MSELoss(reduction='sum') \"\"\"\n",
    "loss = nn.HuberLoss()"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.6. 定义优化算法"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.609698Z",
     "start_time": "2025-02-19T01:33:01.606855Z"
    }
   },
   "source": [
    "\"\"\" trainer = torch.optim.SGD(net.parameters(), lr=0.03) \"\"\"\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=0.03) # if nn.MSELoss(reduction='sum')"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.7 训练"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:33:01.795950Z",
     "start_time": "2025-02-19T01:33:01.653064Z"
    }
   },
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        l = loss(net(X) ,y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        for param in net.parameters():\n",
    "            print(param.grad)\n",
    "        trainer.step()\n",
    "    l = loss(net(features), labels)\n",
    "    print(f'epoch {epoch + 1}, loss {l:f}')\n",
    "\n",
    "w = net[0].weight.data\n",
    "print('w的估计误差：', true_w - w.reshape(true_w.shape))\n",
    "b = net[0].bias.data\n",
    "print('b的估计误差：', true_b - b)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0186,  0.1715]])\n",
      "tensor([-0.8419])\n",
      "tensor([[0.2828, 0.6352]])\n",
      "tensor([-0.8000])\n",
      "tensor([[-0.1669,  0.3447]])\n",
      "tensor([-0.8754])\n",
      "tensor([[-0.6304,  0.7990]])\n",
      "tensor([-0.8000])\n",
      "tensor([[-0.5429, -0.1079]])\n",
      "tensor([-0.8560])\n",
      "tensor([[-0.3561, -0.1918]])\n",
      "tensor([-0.9383])\n",
      "tensor([[-0.3864,  0.1793]])\n",
      "tensor([-0.4000])\n",
      "tensor([[-0.4370,  0.3201]])\n",
      "tensor([-0.8454])\n",
      "tensor([[-0.4337,  0.3788]])\n",
      "tensor([-0.6817])\n",
      "tensor([[-0.3304,  0.1547]])\n",
      "tensor([-0.8712])\n",
      "tensor([[-0.0869, -0.0338]])\n",
      "tensor([-0.8818])\n",
      "tensor([[-0.3747,  0.1630]])\n",
      "tensor([-0.8873])\n",
      "tensor([[-0.3690,  0.5354]])\n",
      "tensor([-0.6075])\n",
      "tensor([[-0.5352,  0.1904]])\n",
      "tensor([-0.6271])\n",
      "tensor([[-0.5364,  0.5515]])\n",
      "tensor([-0.5474])\n",
      "tensor([[-0.2162,  0.1285]])\n",
      "tensor([-1.0000])\n",
      "tensor([[-0.1239,  0.0990]])\n",
      "tensor([-0.6833])\n",
      "tensor([[-0.7789,  0.4829]])\n",
      "tensor([-0.4000])\n",
      "tensor([[-0.5889,  0.4518]])\n",
      "tensor([-0.9220])\n",
      "tensor([[-0.2260, -0.0129]])\n",
      "tensor([-0.9729])\n",
      "tensor([[-0.2294,  0.7945]])\n",
      "tensor([-0.5581])\n",
      "tensor([[-0.1979,  0.9383]])\n",
      "tensor([-0.3941])\n",
      "tensor([[-0.0856,  0.1357]])\n",
      "tensor([-0.8606])\n",
      "tensor([[0.1313, 0.2475]])\n",
      "tensor([-0.8691])\n",
      "tensor([[-0.1657,  0.2507]])\n",
      "tensor([-0.6259])\n",
      "tensor([[0.0300, 0.6690]])\n",
      "tensor([-0.3789])\n",
      "tensor([[-0.2201,  0.3628]])\n",
      "tensor([-0.6729])\n",
      "tensor([[-0.1618,  0.4857]])\n",
      "tensor([-0.5847])\n",
      "tensor([[-0.2130,  0.1288]])\n",
      "tensor([-0.5564])\n",
      "tensor([[-0.3869,  0.4366]])\n",
      "tensor([-0.4220])\n",
      "tensor([[-0.5007,  0.0682]])\n",
      "tensor([-0.7477])\n",
      "tensor([[-0.1223,  0.6213]])\n",
      "tensor([-0.3554])\n",
      "tensor([[0.0216, 0.3139]])\n",
      "tensor([-0.8444])\n",
      "tensor([[0.0653, 0.5118]])\n",
      "tensor([-0.9331])\n",
      "tensor([[0.1217, 0.7214]])\n",
      "tensor([-0.9233])\n",
      "tensor([[0.1609, 0.3200]])\n",
      "tensor([-0.6213])\n",
      "tensor([[-0.2234,  0.0158]])\n",
      "tensor([-1.0000])\n",
      "tensor([[-0.1491,  0.7164]])\n",
      "tensor([-0.5565])\n",
      "tensor([[-0.6762,  0.7895]])\n",
      "tensor([0.])\n",
      "tensor([[-0.7119,  0.1941]])\n",
      "tensor([-0.9860])\n",
      "tensor([[-0.0758,  0.6086]])\n",
      "tensor([-0.7367])\n",
      "tensor([[-0.3856,  0.2642]])\n",
      "tensor([-0.5033])\n",
      "tensor([[-0.4151,  0.3674]])\n",
      "tensor([-0.6861])\n",
      "tensor([[-0.5393,  0.2637]])\n",
      "tensor([-0.7823])\n",
      "tensor([[0.0596, 0.1013]])\n",
      "tensor([-0.6439])\n",
      "tensor([[-0.3567,  0.5900]])\n",
      "tensor([-0.5820])\n",
      "tensor([[-0.0086,  0.5521]])\n",
      "tensor([-0.4839])\n",
      "tensor([[-0.3051,  0.1080]])\n",
      "tensor([-0.9444])\n",
      "tensor([[-0.4790,  0.3010]])\n",
      "tensor([-0.0923])\n",
      "tensor([[-0.0725,  0.5950]])\n",
      "tensor([-0.6470])\n",
      "tensor([[-0.2343,  0.1091]])\n",
      "tensor([-0.7303])\n",
      "tensor([[-0.0114,  0.7097]])\n",
      "tensor([-0.6937])\n",
      "tensor([[-0.5263,  0.4251]])\n",
      "tensor([-0.8000])\n",
      "tensor([[-0.1417,  0.4142]])\n",
      "tensor([-0.4058])\n",
      "tensor([[-0.2371,  0.3460]])\n",
      "tensor([-0.5999])\n",
      "tensor([[-0.2701,  0.5064]])\n",
      "tensor([-0.6131])\n",
      "tensor([[-0.3673,  0.6190]])\n",
      "tensor([-0.9291])\n",
      "tensor([[-0.3299,  0.4575]])\n",
      "tensor([-0.3599])\n",
      "tensor([[0.0898, 0.3842]])\n",
      "tensor([-0.7912])\n",
      "tensor([[-0.0675,  0.1392]])\n",
      "tensor([-0.8234])\n",
      "tensor([[-0.2111,  0.0967]])\n",
      "tensor([-0.8064])\n",
      "tensor([[-0.2743,  0.3582]])\n",
      "tensor([-0.3493])\n",
      "tensor([[-0.1720,  0.7983]])\n",
      "tensor([-0.6000])\n",
      "tensor([[-0.3748,  0.5548]])\n",
      "tensor([-0.5807])\n",
      "tensor([[-0.0893,  0.5827]])\n",
      "tensor([-0.7963])\n",
      "tensor([[-0.3719,  0.1280]])\n",
      "tensor([-0.7475])\n",
      "tensor([[0.2780, 0.2531]])\n",
      "tensor([-0.6368])\n",
      "tensor([[-0.1273,  0.1359]])\n",
      "tensor([-0.5931])\n",
      "tensor([[-0.4508,  0.0715]])\n",
      "tensor([-0.7916])\n",
      "tensor([[-0.1351,  0.7677]])\n",
      "tensor([-0.6000])\n",
      "tensor([[-1.0125,  0.4359]])\n",
      "tensor([-0.2419])\n",
      "tensor([[-0.3931,  0.3577]])\n",
      "tensor([-0.7617])\n",
      "tensor([[-0.5605,  0.5636]])\n",
      "tensor([-0.0756])\n",
      "tensor([[-0.6112,  0.1651]])\n",
      "tensor([-1.0000])\n",
      "tensor([[-0.3318,  0.4315]])\n",
      "tensor([-0.6312])\n",
      "tensor([[-0.0801,  0.4128]])\n",
      "tensor([-0.6801])\n",
      "tensor([[0.2015, 0.3584]])\n",
      "tensor([-0.8079])\n",
      "tensor([[-0.4041,  1.3270]])\n",
      "tensor([-0.3908])\n",
      "tensor([[-0.3343,  0.4098]])\n",
      "tensor([-0.6745])\n",
      "tensor([[-0.0133,  0.3387]])\n",
      "tensor([-0.5038])\n",
      "tensor([[-0.4521,  0.2510]])\n",
      "tensor([-0.4288])\n",
      "tensor([[-0.5871,  0.5820]])\n",
      "tensor([-0.4826])\n",
      "tensor([[-0.1828,  0.6004]])\n",
      "tensor([-0.6253])\n",
      "tensor([[0.0192, 0.1509]])\n",
      "tensor([-0.6722])\n",
      "tensor([[-0.1436,  0.8178]])\n",
      "tensor([-0.2137])\n",
      "tensor([[0.3662, 0.7437]])\n",
      "tensor([-0.7705])\n",
      "tensor([[-0.1116,  0.4199]])\n",
      "tensor([-0.4936])\n",
      "tensor([[-0.6104,  0.6414]])\n",
      "tensor([-0.5197])\n",
      "tensor([[0.0071, 0.5106]])\n",
      "tensor([-0.8000])\n",
      "tensor([[-0.4653,  0.7136]])\n",
      "tensor([-0.9592])\n",
      "tensor([[-0.1290,  0.5946]])\n",
      "tensor([-0.3660])\n",
      "tensor([[-0.1568,  0.3435]])\n",
      "tensor([-0.5201])\n",
      "tensor([[-0.3441,  0.0849]])\n",
      "tensor([-0.6735])\n",
      "tensor([[-0.2664,  0.3532]])\n",
      "tensor([-0.7158])\n",
      "tensor([[-0.1839,  0.2040]])\n",
      "tensor([-0.7250])\n",
      "tensor([[-0.5905,  0.5882]])\n",
      "tensor([-0.8193])\n",
      "tensor([[-0.1890,  0.0358]])\n",
      "tensor([-0.6187])\n",
      "tensor([[-0.1015,  0.6037]])\n",
      "tensor([-0.5854])\n",
      "tensor([[0.0337, 0.0861]])\n",
      "tensor([-0.9605])\n",
      "tensor([[-0.6337,  0.4636]])\n",
      "tensor([-0.8202])\n",
      "epoch 1, loss 2.220514\n",
      "tensor([[-0.2931,  0.0804]])\n",
      "tensor([-0.6109])\n",
      "tensor([[-0.0718,  0.2754]])\n",
      "tensor([-0.9211])\n",
      "tensor([[-0.4545,  0.3133]])\n",
      "tensor([-0.4380])\n",
      "tensor([[-0.1637,  0.8051]])\n",
      "tensor([-0.5006])\n",
      "tensor([[-0.2980,  1.0961]])\n",
      "tensor([-0.8908])\n",
      "tensor([[0.2503, 0.4353]])\n",
      "tensor([-0.7459])\n",
      "tensor([[-0.5195,  0.4523]])\n",
      "tensor([-0.4940])\n",
      "tensor([[-0.5610,  0.6875]])\n",
      "tensor([-0.6812])\n",
      "tensor([[-0.4737,  0.1175]])\n",
      "tensor([-0.5718])\n",
      "tensor([[-0.5295,  0.5193]])\n",
      "tensor([-0.6483])\n",
      "tensor([[-0.0689,  0.3491]])\n",
      "tensor([-0.5520])\n",
      "tensor([[-0.4402,  0.7235]])\n",
      "tensor([-0.8000])\n",
      "tensor([[-0.8332,  0.4444]])\n",
      "tensor([-0.5495])\n",
      "tensor([[-0.3179,  0.3345]])\n",
      "tensor([-0.7564])\n",
      "tensor([[0.0862, 0.2839]])\n",
      "tensor([-0.5038])\n",
      "tensor([[-0.2750,  0.1545]])\n",
      "tensor([-0.9657])\n",
      "tensor([[-0.6240,  0.6581]])\n",
      "tensor([-0.6500])\n",
      "tensor([[-0.1442,  0.1778]])\n",
      "tensor([-0.3919])\n",
      "tensor([[-0.2919,  0.2109]])\n",
      "tensor([-0.6608])\n",
      "tensor([[-0.4152,  0.6248]])\n",
      "tensor([-0.6102])\n",
      "tensor([[0.2780, 0.8485]])\n",
      "tensor([-0.7757])\n",
      "tensor([[-0.2067,  0.5204]])\n",
      "tensor([-0.5278])\n",
      "tensor([[0.0318, 0.6589]])\n",
      "tensor([-0.6315])\n",
      "tensor([[-0.7781,  0.2939]])\n",
      "tensor([-0.7675])\n",
      "tensor([[0.1622, 0.3005]])\n",
      "tensor([-0.7528])\n",
      "tensor([[-0.3094,  0.1976]])\n",
      "tensor([-0.4144])\n",
      "tensor([[-0.6166,  0.5615]])\n",
      "tensor([-0.9264])\n",
      "tensor([[-0.4228,  0.5745]])\n",
      "tensor([-0.4018])\n",
      "tensor([[-0.4155,  0.1460]])\n",
      "tensor([-0.5137])\n",
      "tensor([[-0.0473,  0.6884]])\n",
      "tensor([-0.1221])\n",
      "tensor([[-0.1932,  0.6409]])\n",
      "tensor([-0.1081])\n",
      "tensor([[-0.2092,  0.5303]])\n",
      "tensor([-0.4426])\n",
      "tensor([[-0.4187,  0.2454]])\n",
      "tensor([-0.7198])\n",
      "tensor([[-0.3391,  0.4726]])\n",
      "tensor([-0.9436])\n",
      "tensor([[-0.4085,  0.2794]])\n",
      "tensor([-0.5017])\n",
      "tensor([[-0.3875,  0.2904]])\n",
      "tensor([-0.4227])\n",
      "tensor([[-0.3303,  0.3772]])\n",
      "tensor([-0.8325])\n",
      "tensor([[-0.5041,  0.2911]])\n",
      "tensor([-0.7880])\n",
      "tensor([[-0.0345,  0.8300]])\n",
      "tensor([-0.1556])\n",
      "tensor([[-0.3050,  0.3986]])\n",
      "tensor([-0.5346])\n",
      "tensor([[-0.6878,  0.0907]])\n",
      "tensor([-0.6572])\n",
      "tensor([[-0.1457,  0.2976]])\n",
      "tensor([-0.7460])\n",
      "tensor([[-0.2829,  0.7823]])\n",
      "tensor([-0.0505])\n",
      "tensor([[-0.4674,  0.2879]])\n",
      "tensor([-0.5032])\n",
      "tensor([[-0.5995,  0.1875]])\n",
      "tensor([-0.7242])\n",
      "tensor([[-0.1982,  0.6413]])\n",
      "tensor([-0.6384])\n",
      "tensor([[0.0204, 0.3688]])\n",
      "tensor([-0.7422])\n",
      "tensor([[-0.3056,  0.5058]])\n",
      "tensor([-0.4080])\n",
      "tensor([[-0.4721,  0.4753]])\n",
      "tensor([-0.3498])\n",
      "tensor([[-0.4578,  0.8197]])\n",
      "tensor([-0.4012])\n",
      "tensor([[0.1574, 0.4035]])\n",
      "tensor([-0.4656])\n",
      "tensor([[-0.0307,  0.6702]])\n",
      "tensor([-0.3526])\n",
      "tensor([[-0.4361,  0.5536]])\n",
      "tensor([-0.4283])\n",
      "tensor([[-0.4856,  0.3064]])\n",
      "tensor([-0.6696])\n",
      "tensor([[-0.2587,  0.6824]])\n",
      "tensor([-0.2169])\n",
      "tensor([[-0.3557,  0.9366]])\n",
      "tensor([-0.7485])\n",
      "tensor([[-0.1839,  0.8684]])\n",
      "tensor([-0.4680])\n",
      "tensor([[-0.1438,  0.2714]])\n",
      "tensor([-0.5715])\n",
      "tensor([[-0.5251,  0.3334]])\n",
      "tensor([-0.1998])\n",
      "tensor([[-0.0748,  0.3221]])\n",
      "tensor([-0.8651])\n",
      "tensor([[-0.4038,  0.4804]])\n",
      "tensor([-0.5608])\n",
      "tensor([[-0.2673,  0.3354]])\n",
      "tensor([-0.6463])\n",
      "tensor([[-0.0223,  0.4390]])\n",
      "tensor([-0.2628])\n",
      "tensor([[-0.4836,  0.2789]])\n",
      "tensor([-0.5171])\n",
      "tensor([[-0.4916,  0.7540]])\n",
      "tensor([-0.6033])\n",
      "tensor([[0.3360, 0.8548]])\n",
      "tensor([-0.8333])\n",
      "tensor([[-0.1849,  0.5063]])\n",
      "tensor([-0.5387])\n",
      "tensor([[0.3276, 0.5501]])\n",
      "tensor([-0.9469])\n",
      "tensor([[-0.1348,  0.3281]])\n",
      "tensor([-0.7851])\n",
      "tensor([[-0.1466,  0.3739]])\n",
      "tensor([-0.4557])\n",
      "tensor([[-0.2669,  0.8168]])\n",
      "tensor([-0.3330])\n",
      "tensor([[-0.0646,  0.3541]])\n",
      "tensor([-0.7140])\n",
      "tensor([[-0.3909,  0.7406]])\n",
      "tensor([0.0990])\n",
      "tensor([[-0.3629,  0.2575]])\n",
      "tensor([-0.3389])\n",
      "tensor([[-0.3537,  0.1982]])\n",
      "tensor([-0.3848])\n",
      "tensor([[-0.1118,  0.3956]])\n",
      "tensor([-0.5387])\n",
      "tensor([[-0.2832,  0.2115]])\n",
      "tensor([-0.4497])\n",
      "tensor([[-0.1164,  0.3417]])\n",
      "tensor([-0.3348])\n",
      "tensor([[-0.2195,  0.3559]])\n",
      "tensor([-0.3225])\n",
      "tensor([[-0.1306,  0.5668]])\n",
      "tensor([-0.1774])\n",
      "tensor([[-0.2037,  0.3423]])\n",
      "tensor([-0.6519])\n",
      "tensor([[-0.1697,  0.2401]])\n",
      "tensor([-0.3508])\n",
      "tensor([[0.0891, 0.7200]])\n",
      "tensor([-0.3773])\n",
      "tensor([[-0.7951,  0.4754]])\n",
      "tensor([-0.3004])\n",
      "tensor([[-0.3638,  0.7698]])\n",
      "tensor([-0.3255])\n",
      "tensor([[-0.4488,  0.5785]])\n",
      "tensor([-0.7653])\n",
      "tensor([[-0.0459,  0.2068]])\n",
      "tensor([-0.4126])\n",
      "tensor([[-0.1175,  0.3867]])\n",
      "tensor([-0.4503])\n",
      "tensor([[-0.4430,  1.0324]])\n",
      "tensor([-0.1129])\n",
      "tensor([[-0.3804,  0.5244]])\n",
      "tensor([-0.6699])\n",
      "tensor([[-0.3337,  0.7756]])\n",
      "tensor([-0.4899])\n",
      "tensor([[-0.3363,  0.6616]])\n",
      "tensor([-0.4724])\n",
      "tensor([[-0.1636,  0.4262]])\n",
      "tensor([-0.6778])\n",
      "tensor([[-0.4277,  0.6224]])\n",
      "tensor([-0.2816])\n",
      "tensor([[-0.3318,  0.7340]])\n",
      "tensor([-0.6227])\n",
      "tensor([[-0.5991,  0.2104]])\n",
      "tensor([-0.3473])\n",
      "tensor([[-0.1411,  0.1943]])\n",
      "tensor([-0.2068])\n",
      "tensor([[-0.1741,  0.5247]])\n",
      "tensor([-0.3707])\n",
      "tensor([[0.0195, 0.4868]])\n",
      "tensor([-0.3646])\n",
      "tensor([[-0.5827,  0.5921]])\n",
      "tensor([-0.2433])\n",
      "epoch 2, loss 0.483367\n",
      "tensor([[-0.5074,  0.2584]])\n",
      "tensor([-0.2491])\n",
      "tensor([[-0.1870,  0.2571]])\n",
      "tensor([-0.4679])\n",
      "tensor([[-0.3507,  0.6310]])\n",
      "tensor([-0.3236])\n",
      "tensor([[-0.2982,  0.5385]])\n",
      "tensor([-0.4432])\n",
      "tensor([[-0.4813,  0.5424]])\n",
      "tensor([-0.3263])\n",
      "tensor([[-0.2104,  0.4099]])\n",
      "tensor([-0.5226])\n",
      "tensor([[0.3059, 0.8081]])\n",
      "tensor([-0.1987])\n",
      "tensor([[0.0250, 0.5197]])\n",
      "tensor([-0.7182])\n",
      "tensor([[-0.0072,  0.3684]])\n",
      "tensor([-0.4009])\n",
      "tensor([[-0.1400,  0.2256]])\n",
      "tensor([-0.3059])\n",
      "tensor([[-0.2613,  0.4528]])\n",
      "tensor([-0.5679])\n",
      "tensor([[0.1647, 0.7256]])\n",
      "tensor([-0.6715])\n",
      "tensor([[-0.1238,  0.4326]])\n",
      "tensor([-0.5624])\n",
      "tensor([[-0.4554,  0.2982]])\n",
      "tensor([-0.3362])\n",
      "tensor([[-0.2739,  0.6231]])\n",
      "tensor([-0.3517])\n",
      "tensor([[-0.2657,  0.5368]])\n",
      "tensor([-0.4370])\n",
      "tensor([[0.1456, 0.6786]])\n",
      "tensor([-0.1058])\n",
      "tensor([[-0.0206,  0.6339]])\n",
      "tensor([-0.4198])\n",
      "tensor([[-0.5079,  0.4269]])\n",
      "tensor([-0.3242])\n",
      "tensor([[-0.4105,  0.2986]])\n",
      "tensor([-0.1793])\n",
      "tensor([[-0.5452,  0.5949]])\n",
      "tensor([-0.6373])\n",
      "tensor([[0.0565, 0.3844]])\n",
      "tensor([-0.4187])\n",
      "tensor([[-0.1929,  0.7587]])\n",
      "tensor([0.0912])\n",
      "tensor([[-0.2058,  0.2075]])\n",
      "tensor([-0.2928])\n",
      "tensor([[-0.1244,  0.6007]])\n",
      "tensor([-0.3160])\n",
      "tensor([[0.0238, 0.5107]])\n",
      "tensor([-0.4226])\n",
      "tensor([[-0.0902,  0.2875]])\n",
      "tensor([-0.3026])\n",
      "tensor([[-0.3957,  0.0601]])\n",
      "tensor([-0.2865])\n",
      "tensor([[-0.1940,  0.8833]])\n",
      "tensor([-0.4442])\n",
      "tensor([[-0.6056,  0.5587]])\n",
      "tensor([-0.4731])\n",
      "tensor([[-0.1841,  0.2588]])\n",
      "tensor([-0.2213])\n",
      "tensor([[-0.2398,  0.3551]])\n",
      "tensor([-0.4102])\n",
      "tensor([[-0.1885,  0.4271]])\n",
      "tensor([-0.3521])\n",
      "tensor([[-0.2145,  0.1717]])\n",
      "tensor([-0.1522])\n",
      "tensor([[-0.2036,  0.6513]])\n",
      "tensor([-0.0832])\n",
      "tensor([[-0.1671,  0.3458]])\n",
      "tensor([-0.2927])\n",
      "tensor([[-0.2265,  0.1538]])\n",
      "tensor([-0.2185])\n",
      "tensor([[-0.2994,  0.3239]])\n",
      "tensor([-0.3992])\n",
      "tensor([[-0.2039,  0.3418]])\n",
      "tensor([-0.2880])\n",
      "tensor([[-0.2392,  0.1445]])\n",
      "tensor([-0.1681])\n",
      "tensor([[0.0217, 0.2111]])\n",
      "tensor([-0.1717])\n",
      "tensor([[-0.2288,  0.3245]])\n",
      "tensor([0.0004])\n",
      "tensor([[-0.3199,  0.3045]])\n",
      "tensor([-0.3499])\n",
      "tensor([[-0.2485,  0.2961]])\n",
      "tensor([-0.3112])\n",
      "tensor([[-0.0565,  0.1482]])\n",
      "tensor([-0.1296])\n",
      "tensor([[-0.1300,  0.2065]])\n",
      "tensor([-0.2652])\n",
      "tensor([[-0.1149,  0.5109]])\n",
      "tensor([-0.3037])\n",
      "tensor([[-0.1704,  0.1643]])\n",
      "tensor([-0.1637])\n",
      "tensor([[-0.0297,  0.0215]])\n",
      "tensor([-0.0904])\n",
      "tensor([[-0.1320,  0.2213]])\n",
      "tensor([-0.1827])\n",
      "tensor([[-0.1332,  0.2245]])\n",
      "tensor([-0.2339])\n",
      "tensor([[-0.1663,  0.3242]])\n",
      "tensor([-0.3146])\n",
      "tensor([[-0.2853,  0.2371]])\n",
      "tensor([-0.1306])\n",
      "tensor([[-0.1294,  0.2007]])\n",
      "tensor([-0.1035])\n",
      "tensor([[-0.0792,  0.1744]])\n",
      "tensor([-0.1258])\n",
      "tensor([[-0.0965,  0.2035]])\n",
      "tensor([-0.1823])\n",
      "tensor([[-0.1131,  0.1890]])\n",
      "tensor([-0.0988])\n",
      "tensor([[-0.1777,  0.1824]])\n",
      "tensor([-0.0708])\n",
      "tensor([[-0.0999,  0.0769]])\n",
      "tensor([-0.1108])\n",
      "tensor([[-0.2049,  0.1406]])\n",
      "tensor([-0.0191])\n",
      "tensor([[-0.0656,  0.1446]])\n",
      "tensor([-0.2025])\n",
      "tensor([[-0.1415,  0.1889]])\n",
      "tensor([-0.1588])\n",
      "tensor([[-0.0876,  0.0905]])\n",
      "tensor([-0.1212])\n",
      "tensor([[-0.1913,  0.1324]])\n",
      "tensor([-0.1475])\n",
      "tensor([[-0.0710,  0.0683]])\n",
      "tensor([-0.1258])\n",
      "tensor([[-0.1610,  0.1523]])\n",
      "tensor([-0.1855])\n",
      "tensor([[0.0017, 0.0515]])\n",
      "tensor([-0.0649])\n",
      "tensor([[-0.0621,  0.0825]])\n",
      "tensor([-0.1016])\n",
      "tensor([[-0.1300,  0.1140]])\n",
      "tensor([-0.0818])\n",
      "tensor([[-0.0052,  0.0429]])\n",
      "tensor([-0.0972])\n",
      "tensor([[-0.0452,  0.0175]])\n",
      "tensor([-0.0231])\n",
      "tensor([[-0.0556,  0.1128]])\n",
      "tensor([-0.0604])\n",
      "tensor([[-0.0261,  0.0527]])\n",
      "tensor([-0.0313])\n",
      "tensor([[-0.1077,  0.0284]])\n",
      "tensor([-0.0416])\n",
      "tensor([[-0.0804,  0.1073]])\n",
      "tensor([-0.0994])\n",
      "tensor([[-0.0289,  0.0664]])\n",
      "tensor([0.0015])\n",
      "tensor([[-0.0329,  0.0926]])\n",
      "tensor([-0.0325])\n",
      "tensor([[-0.0082,  0.0258]])\n",
      "tensor([-0.0273])\n",
      "tensor([[-0.0768, -0.0098]])\n",
      "tensor([-0.0635])\n",
      "tensor([[-0.0622,  0.0411]])\n",
      "tensor([-0.0458])\n",
      "tensor([[-0.0034,  0.0294]])\n",
      "tensor([-0.0622])\n",
      "tensor([[-0.0318,  0.1072]])\n",
      "tensor([-0.0827])\n",
      "tensor([[-0.0327,  0.0650]])\n",
      "tensor([-0.0402])\n",
      "tensor([[-0.0229,  0.0151]])\n",
      "tensor([-0.0377])\n",
      "tensor([[-0.0072,  0.0914]])\n",
      "tensor([-0.0867])\n",
      "tensor([[-0.0119,  0.0613]])\n",
      "tensor([-0.0619])\n",
      "tensor([[0.0027, 0.1092]])\n",
      "tensor([-0.0490])\n",
      "tensor([[0.0087, 0.0651]])\n",
      "tensor([-0.0394])\n",
      "tensor([[-0.0211,  0.0578]])\n",
      "tensor([-0.0380])\n",
      "tensor([[-0.0621,  0.0250]])\n",
      "tensor([-0.0326])\n",
      "tensor([[-0.0178,  0.0695]])\n",
      "tensor([-0.0680])\n",
      "tensor([[-0.0893,  0.0432]])\n",
      "tensor([-0.0500])\n",
      "tensor([[-0.0075,  0.0390]])\n",
      "tensor([-0.0542])\n",
      "tensor([[-0.0218,  0.0142]])\n",
      "tensor([-0.0011])\n",
      "tensor([[-0.0353,  0.0010]])\n",
      "tensor([-0.0445])\n",
      "tensor([[-0.0222,  0.0119]])\n",
      "tensor([-0.0321])\n",
      "tensor([[-0.0021,  0.0712]])\n",
      "tensor([-0.0011])\n",
      "tensor([[-0.0076,  0.0175]])\n",
      "tensor([-0.0343])\n",
      "tensor([[-0.0059,  0.0679]])\n",
      "tensor([-0.0512])\n",
      "tensor([[-0.0446,  0.0439]])\n",
      "tensor([-0.0546])\n",
      "epoch 3, loss 0.002535\n",
      "w的估计误差： tensor([ 0.0293, -0.0558])\n",
      "b的估计误差： tensor([0.0366])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "#### 问题解析和解答\n",
    "\n",
    "---\n",
    "\n",
    "##### **1. 如果将小批量的总损失替换为小批量损失的平均值，需要如何更改学习率？**\n",
    "\n",
    "**答：**\n",
    "\n",
    "将小批量的总损失替换为小批量损失的平均值时，梯度的大小会被缩小到原来的 $1 / \\text{batch\\_size}$。为了保持优化过程的稳定性，学习率通常需要相应地调整。\n",
    "\n",
    "- 如果框架没有自动归一化处理梯度，可以 **增大学习率**，通常将学习率乘以 $\\text{batch\\_size}$。\n",
    "- 现代深度学习框架（如 TensorFlow 和 PyTorch）通常已经对梯度进行了归一化，因此无需手动调整学习率。\n",
    "\n",
    "---\n",
    "\n",
    "##### **2. 深度学习框架中提供了哪些损失函数和初始化方法？**\n",
    "\n",
    "**答：**\n",
    "\n",
    "###### **常见损失函数：**\n",
    "\n",
    "- **回归任务**：\n",
    "  - 均方误差损失（Mean Squared Error, MSE）\n",
    "  - 平均绝对误差损失（Mean Absolute Error, MAE）\n",
    "  - Huber 损失（Huber Loss）\n",
    "  - 对数余弦损失（Log-Cosh Loss）\n",
    "  \n",
    "- **分类任务**：\n",
    "  - 交叉熵损失（Cross-Entropy Loss）\n",
    "  - 二分类交叉熵损失（Binary Cross-Entropy Loss）\n",
    "  - Kullback-Leibler 散度（KL Divergence）\n",
    "\n",
    "- **其他**：\n",
    "  - 自定义损失函数\n",
    "\n",
    "###### **常见初始化方法：**\n",
    "\n",
    "- 随机初始化（Random Initialization）\n",
    "- **标准初始化方法**：\n",
    "  - Xavier 初始化（Glorot Initialization）\n",
    "  - He 初始化（Kaiming Initialization）\n",
    "  - 均匀分布初始化（Uniform Distribution）\n",
    "  - 正态分布初始化（Normal Distribution）\n",
    "- 零初始化和常数初始化（Zero and Constant Initialization）\n",
    "\n",
    "###### **Huber 损失公式：**\n",
    "\n",
    "$$\n",
    "l(y, y') =\n",
    "\\begin{cases} \n",
    "|y - y'| - \\frac{\\sigma}{2}, & \\text{if } |y - y'| > \\sigma \\\\ \n",
    "\\frac{1}{2\\sigma}(y - y')^2, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "在实现时，需要设定超参数 $\\sigma$，用以控制线性和二次损失之间的切换点。框架通常会提供默认值，同时允许用户自定义。\n",
    "\n",
    "---\n",
    "\n",
    "##### **3. 如何访问线性回归的梯度？**\n",
    "\n",
    "**答：**\n",
    "\n",
    "在深度学习框架中，线性回归的梯度可以通过自动微分工具（如 TensorFlow 的 `tf.GradientTape` 或 PyTorch 的 `autograd`）进行计算。\n",
    "\n",
    "##### 示例代码如下：\n",
    "\n",
    "##### **PyTorch 示例：**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义模型\n",
    "model = nn.Linear(1, 1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 输入和标签\n",
    "x = torch.tensor([[1.0], [2.0]], requires_grad=True)\n",
    "y = torch.tensor([[2.0], [4.0]])\n",
    "\n",
    "# 前向传播\n",
    "output = model(x)\n",
    "loss = criterion(output, y)\n",
    "\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "\n",
    "# 访问梯度\n",
    "for param in model.parameters():\n",
    "    print(param.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
