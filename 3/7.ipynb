{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:38:14.378416Z",
     "start_time": "2025-02-19T01:38:12.845330Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1. 初始化模型参数"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:38:14.390461Z",
     "start_time": "2025-02-19T01:38:14.384928Z"
    }
   },
   "source": [
    "# PyTorch不会隐式地调整输入的形状。因此，\n",
    "# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状\n",
    "net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights);"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.2. 重新审视Softmax的实现"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:38:14.446629Z",
     "start_time": "2025-02-19T01:38:14.443726Z"
    }
   },
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='none')"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.3. 优化算法"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:38:14.493014Z",
     "start_time": "2025-02-19T01:38:14.489312Z"
    }
   },
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=0.1)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.4. 训练"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_epochs = 10\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "\n",
    "#### 1. 尝试调整超参数，例如批量大小、迭代周期数和学习率，并查看结果。\n",
    "\n",
    "**解答：**\n",
    "\n",
    "调整超参数的影响如下：\n",
    "- **批量大小**：\n",
    "  - 小批量大小：模型更新频率高，能更快收敛，但可能导致更新噪声较大，模型容易陷入局部最优。\n",
    "  - 大批量大小：更新更稳定，但训练效率可能较低，且容易导致欠拟合。\n",
    "\n",
    "- **迭代周期数**：\n",
    "  - 较少的迭代周期：可能导致模型未充分训练，精度不足。\n",
    "  - 较多的迭代周期：可能导致过拟合，模型在训练集上的表现很好，但在测试集上效果变差。\n",
    "\n",
    "- **学习率**：\n",
    "  - 学习率过小：收敛速度慢，可能需要较长时间才能达到最优解。\n",
    "  - 学习率过大：可能导致模型不稳定，甚至无法收敛。\n",
    "\n",
    "优化策略为：\n",
    "- 使用验证集观察模型性能，选择合适的超参数。\n",
    "- 初始时采用较大的学习率，并在训练过程中逐步衰减（如使用学习率调度器）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 增加迭代周期的数量。为什么测试精度会在一段时间后降低？我们怎么解决这个问题？\n",
    "\n",
    "**解答：**\n",
    "\n",
    "- **原因：**\n",
    "  测试精度在一段时间后降低是由于**过拟合**问题。随着迭代周期的增加，模型在训练集上的表现越来越好，但可能会开始过度拟合训练数据中的噪声，从而导致在测试集上的泛化性能下降。\n",
    "\n",
    "- **解决方法：**\n",
    "  1. **早停法（Early Stopping）**：\n",
    "     在训练过程中，持续监控验证集的性能，当验证集的损失在若干迭代后不再下降时停止训练。\n",
    "\n",
    "  2. **正则化**：\n",
    "     - 添加正则化项（如 $L_1$ 或 $L_2$ 正则化）以限制模型的复杂度。\n",
    "     - 增加Dropout层，随机丢弃部分神经元，减少过拟合。\n",
    "\n",
    "  3. **数据增强**：\n",
    "     增加训练数据的多样性（如图像旋转、翻转、缩放等），提高模型的泛化能力。\n",
    "\n",
    "  4. **减少模型复杂度**：\n",
    "     使用更小的网络或减少参数数量，降低模型过拟合的风险。\n",
    "\n",
    "  5. **学习率调整**：\n",
    "     在后期逐步降低学习率，使模型在接近最优解时能够更稳定地收敛。\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
