{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.1. 前向传播\n",
    "![forward.svg](https://zh.d2l.ai/_images/forward.svg \"图4.7.1 前向传播的计算图\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7.3 反向传播\n",
    "反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。 该算法存储了计算某些参数梯度时所需的任何中间变量（偏导数）。 假设我们有函数$\\mathsf{Y}=f(\\mathsf{X})$\n",
    "和$\\mathsf{Z}=g(\\mathsf{Y})$\n",
    "， 其中输入和输出$\\mathsf{X}, \\mathsf{Y}, \\mathsf{Z}$\n",
    "是任意形状的张量。 利用链式法则，我们可以计算$\\mathsf{Z}$\n",
    "关于$\\mathsf{X}$\n",
    "的导数\n",
    "$$\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{X}} = \\text{prod}\\left(\\frac{\\partial \\mathsf{Z}}{\\partial \\mathsf{Y}}, \\frac{\\partial \\mathsf{Y}}{\\partial \\mathsf{X}}\\right).$$\n",
    "在这里，我们使用$\\text{prod}$运算符在执行必要的操作（如换位和交换输入位置）后将其参数相乘。 对于向量，这很简单，它只是矩阵-矩阵乘法。 对于高维张量，我们使用适当的对应项。 运算符$\\text{prod}$\n",
    "指代了所有的这些符号。  \n",
    "回想一下，在前向传播计算图中的单隐藏层简单网络的参数是$\\mathbf{W}^{(1)}$和$\\mathbf{W}^{(1)}$。 反向传播的目的是计算梯度$\\partial J/\\partial \\mathbf{W}^{(1)}$和$\\partial J/\\partial \\mathbf{W}^{(2)}$。 为此，我们应用链式法则，依次计算每个中间变量和参数的梯度。 计算的顺序与前向传播中执行的顺序相反，因为我们需要从计算图的结果开始，并朝着参数的方向努力。第一步是计算目标函数$J = L + s$相对于损失项$L$和正则项$s$的梯度。\n",
    "$$\\frac{\\partial J}{\\partial L} = 1 \\; \\text{and} \\; \\frac{\\partial J}{\\partial s} = 1.$$\n",
    "接下来，我们根据链式法则计算目标函数关于输出层变量$\\mathbf{o}$的梯度：\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{o}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial L}, \\frac{\\partial L}{\\partial \\mathbf{o}}\\right)\n",
    "= \\frac{\\partial L}{\\partial \\mathbf{o}}\n",
    "\\in \\mathbb{R}^q.$$\n",
    "接下来，我们计算正则化项相对于两个参数的梯度：\n",
    "$$\\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}} = \\lambda \\mathbf{W}^{(1)}\n",
    "\\; \\text{and} \\;\n",
    "\\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}} = \\lambda \\mathbf{W}^{(2)}.$$\n",
    "现在我们可以计算最接近输出层的模型参数的梯度$\\partial J/\\partial \\mathbf{W}^{(2)} \\in \\mathbb{R}^{q \\times h}$。 使用链式法则得出：\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(2)}}= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{W}^{(2)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(2)}}\\right)= \\frac{\\partial J}{\\partial \\mathbf{o}} \\mathbf{h}^\\top + \\lambda \\mathbf{W}^{(2)}.$$\n",
    "为了获得关于$\\mathbf{W}^{(1)}$的梯度，我们需要继续沿着输出层到隐藏层反向传播。 关于隐藏层输出的梯度$\\partial J/\\partial \\mathbf{h} \\in \\mathbb{R}^h$由下式给出：\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{h}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{o}}, \\frac{\\partial \\mathbf{o}}{\\partial \\mathbf{h}}\\right)\n",
    "= {\\mathbf{W}^{(2)}}^\\top \\frac{\\partial J}{\\partial \\mathbf{o}}.$$\n",
    "由于激活函数$\\phi$是按元素计算的， 计算中间变量$\\mathbf{z}$的梯度$\\partial J/\\partial \\mathbf{z} \\in \\mathbb{R}^h$需要使用按元素乘法运算符，我们用$\\odot$表示：\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{z}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{h}}, \\frac{\\partial \\mathbf{h}}{\\partial \\mathbf{z}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{h}} \\odot \\phi'\\left(\\mathbf{z}\\right).$$\n",
    "最后，我们可以得到最接近输入层的模型参数的梯度$\\partial J/\\partial \\mathbf{W}^{(1)} \\in \\mathbb{R}^{h \\times d}$。 根据链式法则，我们得到：\n",
    "$$\\frac{\\partial J}{\\partial \\mathbf{W}^{(1)}}\n",
    "= \\text{prod}\\left(\\frac{\\partial J}{\\partial \\mathbf{z}}, \\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}^{(1)}}\\right) + \\text{prod}\\left(\\frac{\\partial J}{\\partial s}, \\frac{\\partial s}{\\partial \\mathbf{W}^{(1)}}\\right)\n",
    "= \\frac{\\partial J}{\\partial \\mathbf{z}} \\mathbf{x}^\\top + \\lambda \\mathbf{W}^{(1)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解答\n",
    "\n",
    "#### 1. 标量函数 **f** 对输入矩阵 **X** 的梯度维数\n",
    "假设 **X** 是一个 $n \\times m$ 的矩阵，**f** 是一个标量函数，则 **f** 对 **X** 的梯度是一个与 **X** 相同维度的矩阵，即梯度维数为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial X} \\in \\mathbb{R}^{n \\times m}\n",
    "$$\n",
    "\n",
    "因为 **f** 对 **X** 的每个元素 $\\partial f / \\partial X_{ij}$ 都会对应一个偏导数。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 添加隐藏层后的计算图和正反向传播公式\n",
    "\n",
    "##### 计算图的绘制\n",
    "假设我们添加一个隐藏层，隐藏层的输出 **H** 由以下公式定义：\n",
    "\n",
    "$$\n",
    "H = g(WX + b)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- **W** 是权重矩阵，维度为 $k \\times n$；\n",
    "- **b** 是偏置向量，维度为 $k \\times 1$；\n",
    "- **g** 是激活函数（如 ReLU、Sigmoid 等），逐元素作用。\n",
    "\n",
    "输出 **O** 为：\n",
    "\n",
    "$$\n",
    "O = f(H)\n",
    "$$\n",
    "\n",
    "计算图的结构如下：\n",
    "- **X** 输入经过矩阵运算 **WX + b**；\n",
    "- 应用激活函数 **g**；\n",
    "- 将结果输入到标量函数 **f**，得到输出。\n",
    "\n",
    "##### 正向传播公式\n",
    "正向传播分为以下步骤：\n",
    "1. 计算加权求和：$Z = WX + b$；\n",
    "2. 应用激活函数：$H = g(Z)$；\n",
    "3. 计算输出：$O = f(H)$。\n",
    "\n",
    "##### 反向传播公式\n",
    "通过链式法则，计算梯度的反向传播：\n",
    "1. 计算 $\\frac{\\partial f}{\\partial H}$；\n",
    "2. 传播至 **Z**：$\\frac{\\partial f}{\\partial Z} = \\frac{\\partial f}{\\partial H} \\odot g'(Z)$，其中 $\\odot$ 表示逐元素乘法；\n",
    "3. 传播至 **W** 和 **b**：\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial W} = \\frac{\\partial f}{\\partial Z} X^T,\n",
    "   \\quad\n",
    "   \\frac{\\partial f}{\\partial b} = \\sum_{i=1}^{m} \\frac{\\partial f}{\\partial Z_{i}}\n",
    "   $$\n",
    "4. 传播至 **X**：\n",
    "   $$\n",
    "   \\frac{\\partial f}{\\partial X} = W^T \\frac{\\partial f}{\\partial Z}\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 内存占用计算\n",
    "对于训练和预测，中间结果会占用内存。需要存储以下内容：\n",
    "1. 输入矩阵 **X**：大小为 $n \\times m$；\n",
    "2. 权重矩阵 **W**：大小为 $k \\times n$；\n",
    "3. 偏置向量 **b**：大小为 $k \\times 1$；\n",
    "4. 中间结果 **Z** 和 **H**：大小均为 $k \\times m$。\n",
    "\n",
    "总内存占用为：\n",
    "\n",
    "$$\n",
    "n \\times m + k \\times n + k \\times 1 + 2 \\times (k \\times m)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 计算图的二阶导数\n",
    "计算二阶导数时，需要计算梯度的梯度，会导致更多计算和内存消耗。以下是可能发生的情况：\n",
    "1. 需要存储和计算二阶导数矩阵（Hessian 矩阵），其大小通常为 $nm \\times nm$，占用内存大幅增加；\n",
    "2. 计算时间复杂度显著提高，可能达到 $O((nm)^2)$，具体时间与矩阵的稀疏性等有关。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 多 GPU 切分与小批量训练的优缺点\n",
    "\n",
    "##### 多 GPU 切分\n",
    "将计算图切分到多个 GPU 上，可通过以下方式实现：\n",
    "1. 将输入数据 **X** 按批次划分，每个 GPU 处理一部分数据；\n",
    "2. 在每个 GPU 上进行独立的前向和反向传播，最后汇总梯度以更新参数。\n",
    "\n",
    "优点：\n",
    "- 并行化提高了计算速度；\n",
    "- 可处理更大规模的数据。\n",
    "\n",
    "缺点：\n",
    "- GPU 之间通信开销较大；\n",
    "- 数据切分可能导致负载不均。\n",
    "\n",
    "##### 与小批量训练对比\n",
    "多 GPU 切分与小批量训练的优缺点对比如下：\n",
    "\n",
    "| **方法**        | **优点**                            | **缺点**                           |\n",
    "|------------------|-------------------------------------|-------------------------------------|\n",
    "| **多 GPU 切分** | 加快计算速度，处理更大模型和数据     | 通信开销高，硬件资源需求更高       |\n",
    "| **小批量训练**  | 内存占用小，易实现                  | 收敛速度较慢，可能需要更多迭代次数 |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
