{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1 重新审视过拟合\n",
    "当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n",
    "\n",
    "泛化性和灵活性之间的这种基本权衡被描述为偏差-方差权衡（bias-variance tradeoff）。 线性模型有很高的偏差：它们只能表示一小类函数。 然而，这些模型的方差很低：它们在不同的随机数据样本上可以得出相似的结果。\n",
    "\n",
    "深度神经网络位于偏差-方差谱的另一端。 与线性模型不同，神经网络并不局限于单独查看每个特征，而是学习特征之间的交互。 例如，神经网络可能推断“尼日利亚”和“西联汇款”一起出现在电子邮件中表示垃圾邮件， 但单独出现则不表示垃圾邮件。\n",
    "\n",
    "即使我们有比特征多得多的样本，深度神经网络也有可能过拟合。 2017年，一组研究人员通过在随机标记的图像上训练深度网络。 这展示了神经网络的极大灵活性，因为人类很难将输入和随机标记的输出联系起来， 但通过随机梯度下降优化的神经网络可以完美地标记训练集中的每一幅图像。 想一想这意味着什么？ 假设标签是随机均匀分配的，并且有10个类别，那么分类器在测试数据上很难取得高于10%的精度， 那么这里的泛化差距就高达90%，如此严重的过拟合。\n",
    "\n",
    "深度网络的泛化性质令人费解，而这种泛化性质的数学基础仍然是悬而未决的研究问题。 我们鼓励喜好研究理论的读者更深入地研究这个主题。 本节，我们将着重对实际工具的探究，这些工具倾向于改进深层网络的泛化性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.4 从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:51:30.947555Z",
     "start_time": "2025-02-19T01:51:29.385063Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "\n",
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <= 1\n",
    "    # 在本情况中，所有元素都被丢弃\n",
    "    if dropout == 1:\n",
    "        return torch.zeros_like(X)\n",
    "    # 在本情况中，所有元素都被保留\n",
    "    if dropout == 0:\n",
    "        return X\n",
    "    mask = (torch.rand(X.shape) > dropout).float()\n",
    "    return mask * X / (1.0 - dropout)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:51:30.962116Z",
     "start_time": "2025-02-19T01:51:30.955191Z"
    }
   },
   "source": [
    "X= torch.arange(16, dtype = torch.float32).reshape((2, 8))\n",
    "print(X)\n",
    "print(dropout_layer(X, 0.))\n",
    "print(dropout_layer(X, 0.5))\n",
    "print(dropout_layer(X, 1.))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11., 12., 13., 14., 15.]])\n",
      "tensor([[ 0.,  2.,  4.,  6.,  8., 10., 12.,  0.],\n",
      "        [ 0.,  0., 20., 22.,  0.,  0.,  0.,  0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:51:31.011177Z",
     "start_time": "2025-02-19T01:51:31.008366Z"
    }
   },
   "source": [
    "num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:51:31.059775Z",
     "start_time": "2025-02-19T01:51:31.054020Z"
    }
   },
   "source": [
    "dropout1, dropout2 = 0.2, 0.5\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2,\n",
    "                 is_training = True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_inputs = num_inputs\n",
    "        self.training = is_training\n",
    "        self.lin1 = nn.Linear(num_inputs, num_hiddens1)\n",
    "        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)\n",
    "        self.lin3 = nn.Linear(num_hiddens2, num_outputs)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))\n",
    "        # 只有在训练模型时才使用dropout\n",
    "        if self.training == True:\n",
    "            # 在第一个全连接层之后添加一个dropout层\n",
    "            H1 = dropout_layer(H1, dropout1)\n",
    "        H2 = self.relu(self.lin2(H1))\n",
    "        if self.training == True:\n",
    "            # 在第二个全连接层之后添加一个dropout层\n",
    "            H2 = dropout_layer(H2, dropout2)\n",
    "        out = self.lin3(H2)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "num_epochs, lr, batch_size = 10, 0.5, 256\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)\n",
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.5 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T01:51:53.482452Z",
     "start_time": "2025-02-19T01:51:53.476428Z"
    }
   },
   "source": [
    "net = nn.Sequential(nn.Flatten(),\n",
    "        nn.Linear(784, 256),\n",
    "        nn.ReLU(),\n",
    "        # 在第一个全连接层之后添加一个dropout层\n",
    "        nn.Dropout(dropout1),\n",
    "        nn.Linear(256, 256),\n",
    "        nn.ReLU(),\n",
    "        # 在第二个全连接层之后添加一个dropout层\n",
    "        nn.Dropout(dropout2),\n",
    "        nn.Linear(256, 10))\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=0.01)\n",
    "\n",
    "net.apply(init_weights);"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "trainer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice\n",
    "### 1. 如果更改第一层和第二层的暂停法概率，会发生什么情况？具体地说，如果交换这两个层，会发生什么情况？设计一个实验来回答这些问题，定量描述该结果，并总结性的结论。\n",
    "\n",
    "**实验设计**：\n",
    "- 设置两层神经网络，并分别设定第一层和第二层的暂停概率（Dropout Rate）。\n",
    "- 两组实验：\n",
    "  1. 第一层暂停率为 $p_1$，第二层暂停率为 $p_2$；\n",
    "  2. 交换两层的暂停率，即第一层为 $p_2$，第二层为 $p_1$。\n",
    "- 记录训练和测试的误差，并观察两种设置的性能。\n",
    "\n",
    "**结果总结**：\n",
    "1. 通常情况下，较高的暂停率对靠近输入层的影响更大，因为输入层特征受损可能导致后续层学习不稳定。\n",
    "2. 当交换两层暂停率时，模型性能可能有所下降，尤其在第一层暂停率过高时。\n",
    "3. 两个层的暂停概率需要根据具体问题调优，不能单纯地交换。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. 增加训练轮数，并将使用暂停法和不使用暂停法时获得的结果进行比较。\n",
    "\n",
    "**分析方法**：\n",
    "- 在相同训练轮数下，比较使用和不使用暂停法的训练和测试误差。\n",
    "- 逐渐增加训练轮数，观察模型的收敛趋势。\n",
    "\n",
    "**结果总结**：\n",
    "1. **使用暂停法**：\n",
    "   - 初期的训练误差较高，但随着训练轮数增加，模型对测试集的性能逐渐提高，表现为更好的泛化能力。\n",
    "2. **不使用暂停法**：\n",
    "   - 训练误差迅速下降，但测试误差可能较高，表明模型容易过拟合。\n",
    "3. **结论**：\n",
    "   - 暂停法通过随机丢弃部分神经元，强制模型在有限的特征下学习，增加了泛化能力，尤其在训练轮数较多时表现出优势。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 当前用或不用暂停法时，每个隐藏层中激活值的方差是多少？给出一个曲线图，以显示这两个模型中每个隐藏层中激活值的方差是如何随时间变化的。\n",
    "\n",
    "**分析方法**：\n",
    "- 记录每个隐藏层激活值的方差，分别在使用和不使用暂停法的情况下比较。\n",
    "- 绘制激活值方差随训练轮数变化的曲线。\n",
    "\n",
    "**结果总结**：\n",
    "- **使用暂停法**：激活值的方差较平稳，表明模型的特征表达更加稳定。\n",
    "- **不使用暂停法**：激活值的方差可能较大，且波动较为明显，尤其在深层网络中更为显著。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 为什么在测试时通常不使用暂停法？\n",
    "\n",
    "在测试阶段，模型需要评估性能，暂停法会随机丢弃神经元，导致模型输出不稳定。因此，测试时通常不使用暂停法，而是将训练期间的丢弃率通过缩放权重来反映。在测试阶段：\n",
    "- 每个神经元的输出会乘以 $1 - p$（其中 $p$ 是暂停概率）。\n",
    "- 这样可以模拟训练阶段的期望输出，保持一致性。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 以本节中的模型为例，比较使用暂停法和权重衰减的效果。如果同时使用暂停法和权重衰减，会发生什么情况？结果是累加的吗？收敛是否减少（或者说更糟）？它们互相抵消了吗？\n",
    "\n",
    "**分析**：\n",
    "- **单独使用暂停法**：通过随机丢弃神经元来减少过拟合，增强泛化能力。\n",
    "- **单独使用权重衰减**：通过在损失函数中加入权重的正则化项，抑制过大的权重值。\n",
    "- **同时使用**：\n",
    "  1. 两种方法可以互补，权重衰减控制参数规模，暂停法增强模型的鲁棒性。\n",
    "  2. 模型可能需要更长的训练时间才能收敛。\n",
    "  3. 如果参数选择不当（如权重衰减系数过大），可能会导致相互抵消，结果变得更差。\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 如果我们将暂停法应用到权重矩阵的各个权重，而不是激活值，会发生什么？\n",
    "\n",
    "如果将暂停法应用于权重矩阵的各个权重：\n",
    "- **效果**：\n",
    "  1. 权重矩阵中部分权重被随机置零，导致权重更新不完整。\n",
    "  2. 可能引入额外的梯度噪声，训练过程更加不稳定。\n",
    "- **潜在问题**：\n",
    "  - 权重矩阵中的零值可能阻断信息流，尤其对深层网络影响更大。\n",
    "\n",
    "这种方法通常不如对激活值进行暂停法有效，但可以通过实验比较其性能。\n",
    "\n",
    "---\n",
    "\n",
    "### 7. 发明一种用于在每一层注入随机噪声的技术，该技术不同于标准的暂停法技术。尝试开发一种在Fashion-MNIST数据集（对于固定架构）上性能优于暂停法的方法。\n",
    "\n",
    "**新技术：激活值扰动法（Activation Perturbation）**：\n",
    "- 在每一层的激活值上添加小幅随机噪声：\n",
    "  $$\n",
    "  h' = h + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "  $$\n",
    "  其中，$\\epsilon$ 是服从零均值正态分布的随机噪声。\n",
    "\n",
    "**执行步骤**：\n",
    "1. 在每一前向传播中，对每层的激活值添加随机噪声。\n",
    "2. 调整噪声强度 $\\sigma$，确保噪声不会过大以破坏特征表达。\n",
    "\n",
    "**实验验证**：\n",
    "- 在Fashion-MNIST数据集上测试该方法，与标准暂停法（Dropout）进行对比。\n",
    "- 观察训练误差和测试误差。\n",
    "\n",
    "**结果预期**：\n",
    "- 激活值扰动法通过增加随机性，防止网络过拟合，同时避免了暂停法中神经元完全丢弃的问题。\n",
    "- 若调整得当，可能在某些任务上优于标准暂停法。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
